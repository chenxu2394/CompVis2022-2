{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true;\n",
       "function code_toggle() {\n",
       "if (code_show){\n",
       "$('div.input').hide();\n",
       "} else {\n",
       "$('div.input').show();\n",
       "}\n",
       "code_show = !code_show\n",
       "}\n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell is used for creating a button that hides/unhides code cells to quickly look only the results.\n",
    "# Works only with Jupyter Notebooks.\n",
    "\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true;\n",
    "function code_toggle() {\n",
    "if (code_show){\n",
    "$('div.input').hide();\n",
    "} else {\n",
    "$('div.input').show();\n",
    "}\n",
    "code_show = !code_show\n",
    "}\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n",
      "Data stored in /coursedata/exercise-07-data\n"
     ]
    }
   ],
   "source": [
    "# Description:\n",
    "#   Exercise7 notebook.\n",
    "#\n",
    "# Copyright (C) 2018 Santiago Cortes, Juha Ylioinas\n",
    "#\n",
    "# This software is distributed under the GNU General Public \n",
    "# Licence (version 2 or later); please refer to the file \n",
    "# Licence.txt, included with the software, for details.\n",
    "\n",
    "# Preparations\n",
    "import numpy as np\n",
    "import os\n",
    "# Select data directory\n",
    "if os.path.isdir('/coursedata'):\n",
    "    # JupyterHub\n",
    "    course_data_dir = '/coursedata'\n",
    "elif os.path.isdir('../../../coursedata'):\n",
    "    # Local installation\n",
    "    course_data_dir = '../../../coursedata'\n",
    "else:\n",
    "    # Docker\n",
    "    course_data_dir = '/home/jovyan/work/coursedata/'\n",
    "\n",
    "print('The data directory is %s' % course_data_dir)\n",
    "data_dir = os.path.join(course_data_dir, 'exercise-07-data')\n",
    "print('Data stored in %s' % data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-E4850 Computer Vision Exercise Round 7\n",
    "The problems should be solved before the exercise session and solutions returned via\n",
    "MyCourses. <br><br> For this exercise round, upload this notebook(pdf and .ipynb versions) containing your source codes (for Exercise 1) and your answer to the question of Exercise2, and all the answers to the questions of Exercise 3 (VGG practical), see part[1-3].ipynb. Note that it's not necessary to upload part1.ipynb, part2.ipynb or part3.ipynb, because all of the necessary questions related to them are contained in this notebook and you're not expected to do any coding in Exercises 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Comparing  bags-of-words  with  tf-idf  weighting\n",
    "Assume  that  we  have  an  indexed  collection  of  documents  containing  the  five  terms  of the following table where the second row indicates the percentage of documents in which each term appears.<br>\n",
    "\n",
    "| term | cat | dog |mammals | mouse | pet |\n",
    "| --- | :---: | :---: | :---: | :---: | :---: |\n",
    "| **% of documents** | 5 | 20 | 2 | 10 | 60 |\n",
    "\n",
    "Now, given the query $Q=\\{mouse, cat, pet, mammals\\}$, compute the similarity between $Q$ and the following example documents $D1$, $D2$, $D3$, by using the cosine similarity measure and tf-idf weights (i.e. term frequency - inverse document frequency) for the bag-of-words histogram representations of the documents and the query.\n",
    "\n",
    "-  $D1$ = Cat is a pet, dog is a pet, and mouse may be a pet too.\n",
    "-  $D2$ = Cat, dog and mouse are all mammals.\n",
    "-  $D3$ = Cat and dog get along well, but cat may eat a mouse.\n",
    "\n",
    "Ignore other words except the five terms. You may proceed with the following steps:\n",
    "\n",
    "a) Compute and report the inverse document frequency (idf) for each of the five terms. Use the logarithm with base 2. (idf is the logarithm on slide 69 of Lecture 6.)<br>\n",
    "b) Compute the term frequencies for the query and each document. <br>\n",
    "c) Form the tf-idf weighted word occurrence histograms for the query and documents. <br>\n",
    "d) Evaluate the cosine similarity between the query and each document (i.e.\\ normalized scalar product between the weighted occurrence histograms as shown on slide 45).<br> \n",
    "e) Report the relative ranking of the documents. (You should get similarities 0.95, 0.64, and 0.63, but you need to determine which corresponds to which document.)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Q and D1 is  0.63\n",
      "Similarity between Q and D2 is  0.95\n",
      "Similarity between Q and D3 is  0.64\n"
     ]
    }
   ],
   "source": [
    "## Comparing  bags-of-words  with  tf-idf  weighting\n",
    "##--your-code-starts-here--##\n",
    "# a) Compute and report the inverse document frequency (idf) for each of the five terms. Use the logarithm with base 2. (idf is the logarithm on slide 69 of Lecture 6.)\n",
    "idf = {}\n",
    "idf[\"cat\"] = np.log2(100/5)\n",
    "idf[\"dog\"] = np.log2(100/20)\n",
    "idf[\"mammals\"] = np.log2(100/2)\n",
    "idf[\"mouse\"] = np.log2(100/10)\n",
    "idf[\"pet\"] = np.log2(100/60)\n",
    "\n",
    "# b) Compute the term frequencies for the query and each document. \n",
    "tf_Q = {}; tf_D1={}; tf_D2={}; tf_D3={}\n",
    "tf_Q [\"mouse\"] = 1/4;  tf_Q [\"cat\"] = 1/4;  tf_Q [\"pet\"] = 1/4;  tf_Q [\"mammals\"] = 1/4;  tf_Q [\"dog\"] = 0\n",
    "tf_D1[\"mouse\"] = 1/15; tf_D1[\"cat\"] = 1/15; tf_D1[\"pet\"] = 3/15; tf_D1[\"mammals\"] = 0/15; tf_D1[\"dog\"] = 1/15\n",
    "tf_D2[\"mouse\"] = 1/7;  tf_D2[\"cat\"] = 1/7;  tf_D2[\"pet\"] = 0/7;  tf_D2[\"mammals\"] = 1/7;  tf_D2[\"dog\"] = 1/7\n",
    "tf_D3[\"mouse\"] = 1/12; tf_D3[\"cat\"] = 2/12; tf_D3[\"pet\"] = 0/12; tf_D3[\"mammals\"] = 0/12; tf_D3[\"dog\"] = 1/12\n",
    "\n",
    "#c) Form the tf-idf weighted word occurrence histograms for the query and documents.\n",
    "tfidf_Q  = np.array([ tf_Q[\"mouse\"]*idf[\"mouse\"],  tf_Q[\"cat\"]*idf[\"cat\"],  tf_Q[\"pet\"]*idf[\"pet\"],  tf_Q[\"mammals\"]*idf[\"mammals\"],  tf_Q[\"dog\"]*idf[\"dog\"]])\n",
    "tfidf_D1 = np.array([tf_D1[\"mouse\"]*idf[\"mouse\"], tf_D1[\"cat\"]*idf[\"cat\"], tf_D1[\"pet\"]*idf[\"pet\"], tf_D1[\"mammals\"]*idf[\"mammals\"], tf_D1[\"dog\"]*idf[\"dog\"]])\n",
    "tfidf_D2 = np.array([tf_D2[\"mouse\"]*idf[\"mouse\"], tf_D2[\"cat\"]*idf[\"cat\"], tf_D2[\"pet\"]*idf[\"pet\"], tf_D2[\"mammals\"]*idf[\"mammals\"], tf_D2[\"dog\"]*idf[\"dog\"]])\n",
    "tfidf_D3 = np.array([tf_D3[\"mouse\"]*idf[\"mouse\"], tf_D3[\"cat\"]*idf[\"cat\"], tf_D3[\"pet\"]*idf[\"pet\"], tf_D3[\"mammals\"]*idf[\"mammals\"], tf_D3[\"dog\"]*idf[\"dog\"]])\n",
    "\n",
    "#d) Evaluate the cosine similarity between the query and each document (i.e.\\ normalized scalar product between the weighted occurrence histograms as shown on slide 45).\n",
    "sim_Q_D1 = tfidf_Q@tfidf_D1 / (np.linalg.norm(tfidf_Q)*np.linalg.norm(tfidf_D1))\n",
    "sim_Q_D2 = tfidf_Q@tfidf_D2 / (np.linalg.norm(tfidf_Q)*np.linalg.norm(tfidf_D2))\n",
    "sim_Q_D3 = tfidf_Q@tfidf_D3 / (np.linalg.norm(tfidf_Q)*np.linalg.norm(tfidf_D3))\n",
    "\n",
    "#e) Report the relative ranking of the documents. (You should get similarities 0.95, 0.64, and 0.63, but you need to determine which corresponds to which document.)\n",
    "print(f'Similarity between Q and D1 is {sim_Q_D1: .2f}')\n",
    "print(f'Similarity between Q and D2 is {sim_Q_D2: .2f}')\n",
    "print(f'Similarity between Q and D3 is {sim_Q_D3: .2f}')\n",
    "\n",
    "##--your-code-ends-here--##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Precision  and  recall\n",
    "There is a database of 10000 images and a user, who is only interested in images which contain a car. It is known that there are 500 such images in the database. An  automatic image retrieval system retrieves 300 car images and 50 other images from the database. Determine and report the precision and recall of the retrieval  system in this particular case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type your answer here:\n",
    "\n",
    "Precision = 300 / (300+50) = 0.86\n",
    "\n",
    "Recall    = 300 / 500      = 0.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - VGG practical on object instance recognition\n",
    "See the questions in part[1-3].ipynb and write your answers here.\n",
    "\n",
    "Part1:\n",
    "Stage I.A (two questions)\n",
    "Stage I.B (two questions)\n",
    "Stage I.C (one question)\n",
    "\n",
    "Part2 (one question)\n",
    "\n",
    "Part3:\n",
    "Stage III.A (three questions)\n",
    "Stage III.B (one question)\n",
    "Stage III.C (two questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type your answers here: \n",
    "\n",
    "## Part1:\n",
    "\n",
    "### Stage I.A (two questions)\n",
    "\n",
    "**Question**: Note the change in density of detections across the image. Why does it change? Will it be a problem for matching? How could it be avoided?\n",
    "\n",
    "*Because the second image has different illumination condition.\n",
    "It will be a problem for matching.*\n",
    "\n",
    "**Question**: Occasionally, a feature is detected multiple times, with different orientations. This may happen when the orientation assignment is ambiguous. Which kind of image structure would result in ambiguous orientation assignment?\n",
    "\n",
    "*Mono color region will result in ambiguous orientation.*\n",
    "\n",
    "### Stage I.B (two questions)\n",
    " \n",
    "**Question**: Note the descriptors are computed over a much larger region (shown in blue) than the detection (shown in green). Why?\n",
    "\n",
    "*Computation over larger region helps to reduce ambiguity and mismatch.*\n",
    "\n",
    "**Question**: Notice that there are many mismatches. Examine some of the mismatches to understand why the mistakes are being made. For example, is the change in lighting a problem? What additional constraints can be applied to remove the mismatches?\n",
    "\n",
    "### Stage I.C (one question)\n",
    "\n",
    "**Question**: Examine some of the remaining mismatches to understand why they have occurred. How could they be removed?\n",
    "\n",
    "## Part2 (one question)\n",
    "\n",
    "**Question**: The transformation between the images induced by the plane is a planar homography. The detections are only affine co-variant (not as general as a planar homography). So how can descriptors computed on these detections possibly match?\n",
    "\n",
    "\n",
    "## Part3: \n",
    "\n",
    "### Stage III.A (three questions) \n",
    "\n",
    "**Questions**:\n",
    "\n",
    "The size of the vocabulary (the number of clusters) is an important parameter in visual word algorithms. How does the size affect the number of inliers and the difficulty of computing the transformation?\n",
    "\n",
    "*Larger size of the vocabulary will increase the difficulty of computing.*\n",
    "\n",
    "In the above procedure the time required to convert the descriptors into visual words was not accounted for. Why?\n",
    "\n",
    "*Because the time to convert the dscriptors into visual words can be done beforehand.*\n",
    "\n",
    "What is the speedup in searching a large, fixed database of 10, 100, 1000 images?\n",
    "\n",
    "Hint: You may use the lineprofiler (commented lines) to better see what is happening behind the curtains (uncomment and compare Total times). The timings reported in the titles of the plots are not very truthful, but there is a difference between those numbers as well).\n",
    "\n",
    "\n",
    "### Stage III.B (one question) \n",
    "\n",
    "Task: How many erroneously matched images do you count in the top results? \n",
    "\n",
    "*14*\n",
    "\n",
    "**Question**: Why does the top image have a score of 1 (0.9698... see the NOTE above)?\n",
    "\n",
    "*Because they are the same image.*\n",
    "\n",
    "\n",
    "### Stage III.C (two questions)\n",
    "\n",
    "**Question**: Why is the top score much larger than 1 now?\n",
    "\n",
    "*They are now re-scored based on the number of inlier matches after geometric verification.*\n",
    "\n",
    "**Question**: Are the retrieval results improved after geometric verification?\n",
    "\n",
    "*Yes.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
